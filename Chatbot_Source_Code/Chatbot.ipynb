{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwWR1aTek186idZJwZQMk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyushmanBhatt/AI-Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.python.util import deprecation  # To remove tensorflow deprecation warnings\n",
        "import warnings"
      ],
      "metadata": {
        "id": "FQ2qfdjlGqBw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "03ltf9k4GuuZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class modelTrain:\n",
        "    def __init__(self):\n",
        "        self.words = []\n",
        "        self.classes = []\n",
        "        self.documents = []\n",
        "        self.ignore_words = ['?', '!']"
      ],
      "metadata": {
        "id": "i9uD6GRzICyW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadIntents(self, intents_path=''):\n",
        "        data_file = open(intents_path).read()\n",
        "        intents = json.loads(data_file)\n",
        "        return intents\n"
      ],
      "metadata": {
        "id": "r_E-pSuEIE14"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_save_Data(self, intents):\n",
        "        for intent in intents['intents']:\n",
        "            for pattern in intent['patterns']:\n",
        "\n",
        "                # tokenize each word\n",
        "                w = nltk.word_tokenize(pattern)\n",
        "                self.words.extend(w)\n",
        "                # add documents in the corpus\n",
        "                self.documents.append((w, intent['tag']))\n",
        "\n",
        "                # add to our classes list\n",
        "                if intent['tag'] not in self.classes:\n",
        "                    self.classes.append(intent['tag'])\n",
        "\n",
        "        # lemmatize and lower each word and remove duplicates\n",
        "        self.words = [lemmatizer.lemmatize(w.lower()) for w in self.words if w not in self.ignore_words]\n",
        "        self.words = sorted(list(set(self.words)))\n",
        "\n",
        "        # sort classes\n",
        "        self.classes = sorted(list(set(self.classes)))\n",
        "\n",
        "        # documents = combination between patterns and intents\n",
        "        print(len(self.documents), \" documents \")\n",
        "\n",
        "        # classes = intents\n",
        "        print(len(self.classes), \" classes \", self.classes)\n",
        "\n",
        "        # words = all words, vocabulary\n",
        "        print(len(self.words), \" unique lemmatized words \", self.words)\n",
        "\n",
        "        # Save data\n",
        "        pickle.dump(self.words, open('words.pkl', 'wb'))\n",
        "        pickle.dump(self.classes, open('classes.pkl', 'wb'))\n",
        "\n",
        "        return self.words, self.classes"
      ],
      "metadata": {
        "id": "XFOZ_UX-ILk3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def prepareTrainingData(self, words, classes):\n",
        "        # create training data\n",
        "        training = []\n",
        "\n",
        "        # empty output array\n",
        "        output_empty = [0] * len(classes)\n",
        "\n",
        "        # training set, bag of words for each sentence\n",
        "        for doc in self.documents:\n",
        "            # initialize our bag of words\n",
        "            bag = []\n",
        "\n",
        "            # list of tokenized words for the pattern\n",
        "            pattern_words = doc[0]\n",
        "\n",
        "            # lemmatize each word - create base word, in attempt to represent related words\n",
        "            pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "            # create our bag of words array with 1, if word match found in current pattern\n",
        "            for w in words:\n",
        "                bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "            # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "            output_row = list(output_empty)\n",
        "            output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "            training.append([bag, output_row])\n",
        "            # shuffle our features and turn into np.array\n",
        "        random.shuffle(training)\n",
        "        training = np.array(training)\n",
        "\n",
        "        # create train and test lists. X - patterns, Y - intents\n",
        "        train_x = list(training[:, 0])\n",
        "        train_y = list(training[:, 1])\n",
        "        # print(\"Training data created\")\n",
        "\n",
        "        return train_x, train_y"
      ],
      "metadata": {
        "id": "hud-yNsEIefX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createModel(self, train_x, train_y, epochs=200, batch_size=5, save_path='model.model'):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "        # fitting and saving the model\n",
        "        hist = model.fit(np.array(train_x), np.array(train_y), epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "        model.save(save_path, hist)\n",
        "        print(\"Model Successfully Created and saved\")\n",
        "        return model"
      ],
      "metadata": {
        "id": "TTd0WanIIhZ-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class modelPredict:\n",
        "    def __init__(self, intents_path='filename.json', model_path='model_name.json'):\n",
        "        self.intents_path = intents_path\n",
        "        self.model = model_path\n",
        "\n",
        "    def clean_up_sentence(self, sentence):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        # tokenize the pattern - split words into array\n",
        "        sentence_words = nltk.word_tokenize(sentence)\n",
        "\n",
        "        # stem each word - create short form for word\n",
        "        sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "        return sentence_words\n",
        "\n",
        "    # return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "    def bow(self, sentence, words, show_details=False):\n",
        "        # tokenize the pattern\n",
        "        sentence_words = self.clean_up_sentence(sentence)\n",
        "\n",
        "        # bag of words - matrix of N words, vocabulary matrix\n",
        "        bag = [0] * len(words)\n",
        "        for s in sentence_words:\n",
        "            for i, w in enumerate(words):\n",
        "                if w == s:\n",
        "                    # assign 1 if current word is in the vocabulary position\n",
        "                    bag[i] = 1\n",
        "                    if show_details:\n",
        "                        print(\"Found in bag: %s\" % w)\n",
        "        return np.array(bag)\n",
        "\n",
        "    def predict_class(self, sentence, model, error_threshold=0.25):\n",
        "        ERROR_THRESHOLD = error_threshold\n",
        "        words = pickle.load(open('words.pkl', 'rb'))\n",
        "        classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "        # filter out predictions below a threshold\n",
        "        p = self.bow(sentence, words, show_details=False)\n",
        "        res = model.predict(np.array([p]))[0]\n",
        "        # ERROR_THRESHOLD = 0.25\n",
        "        results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "\n",
        "        # sort by strength of probability\n",
        "        results.sort(key=lambda x: x[1], reverse=True)\n",
        "        return_list = []\n",
        "        for r in results:\n",
        "            return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "        return return_list\n",
        "\n",
        "    def getResponse(self, ints, intents_json):\n",
        "        import random\n",
        "        tag = ints[0]['intent']\n",
        "        list_of_intents = intents_json['intents']\n",
        "        for i in list_of_intents:\n",
        "            if i['tag'] == tag:\n",
        "                result = random.choice(i['responses'])\n",
        "                break\n",
        "        return result\n",
        "\n",
        "    def chatbot_response(self, msg):\n",
        "      \n",
        "        model = load_model(self.model)\n",
        "        intents = json.loads(open(self.intents_path).read())\n",
        "        ints = self.predict_class(msg, model)\n",
        "        res = self.getResponse(ints, intents)\n",
        "        return res\n",
        "    # response_from_bot = chatbot_response(input_query)"
      ],
      "metadata": {
        "id": "cr-mGSPEItEf"
      },
      "execution_count": 43,
      "outputs": []
    }
  ]
}
